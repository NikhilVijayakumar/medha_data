# **Q5.** 
> A data analyst is working with a $2 \times 2$ covariance matrix:
    $A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}$

>The analyst needs to perform eigenvalue decomposition to understand the matrix's properties and apply it to data transformation.
 
> 1. Find the eigenvalues of the matrix A.
 
> 2. Find the eigenvectors corresponding to the eigenvalues.
 
> 3. Perform eigenvalue decomposition: represent A in the form $A = Q\Lambda Q^\top$ where:
    * $\Lambda$ is the diagonal matrix of eigenvalues.
    * Q is the orthogonal matrix of eigenvectors.

> 4 Explain how eigenvalue decomposition can be used for dimensionality reduction in machine learning.
---

# Solution

Here's the step-by-step solution for the eigenvalue decomposition of the given covariance matrix and its application in dimensionality reduction:

**Given Covariance Matrix:**
$A = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}$

---

### 1. Find the eigenvalues of the matrix A.

To find the eigenvalues ($\lambda$), we solve the characteristic equation $\text{det}(A - \lambda I) = 0$, where $I$ is the identity matrix.

$A - \lambda I = \begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix} - \lambda \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = \begin{bmatrix} 4-\lambda & 2 \\ 2 & 3-\lambda \end{bmatrix}$

Now, compute the determinant:
$\text{det}(A - \lambda I) = (4-\lambda)(3-\lambda) - (2)(2)$
$= 12 - 4\lambda - 3\lambda + \lambda^2 - 4$
$= \lambda^2 - 7\lambda + 8$

Set the characteristic equation to zero:
$\lambda^2 - 7\lambda + 8 = 0$

We can use the quadratic formula $\lambda = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}$ to find the roots (eigenvalues):
Here, $a=1$, $b=-7$, $c=8$.
$\lambda = \frac{-(-7) \pm \sqrt{(-7)^2 - 4(1)(8)}}{2(1)}$
$\lambda = \frac{7 \pm \sqrt{49 - 32}}{2}$
$\lambda = \frac{7 \pm \sqrt{17}}{2}$

So, the eigenvalues are:
$\lambda_1 = \frac{7 + \sqrt{17}}{2} \approx \frac{7 + 4.123}{2} \approx \frac{11.123}{2} \approx \mathbf{5.5615}$
$\lambda_2 = \frac{7 - \sqrt{17}}{2} \approx \frac{7 - 4.123}{2} \approx \frac{2.877}{2} \approx \mathbf{1.4385}$

---

### 2. Find the eigenvectors corresponding to the eigenvalues.

To find the eigenvectors ($v$), we solve the equation $(A - \lambda I)v = 0$ for each eigenvalue.

#### For $\lambda_1 = \frac{7 + \sqrt{17}}{2}$:
Let $v_1 = \begin{bmatrix} x \\ y \end{bmatrix}$.
$\begin{bmatrix} 4-\lambda_1 & 2 \\ 2 & 3-\lambda_1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

Substitute $\lambda_1 = \frac{7 + \sqrt{17}}{2}$:
$4 - \frac{7 + \sqrt{17}}{2} = \frac{8 - 7 - \sqrt{17}}{2} = \frac{1 - \sqrt{17}}{2}$
$3 - \frac{7 + \sqrt{17}}{2} = \frac{6 - 7 - \sqrt{17}}{2} = \frac{-1 - \sqrt{17}}{2}$

The system of equations becomes:
$\frac{1 - \sqrt{17}}{2}x + 2y = 0 \quad (*)$
$2x + \frac{-1 - \sqrt{17}}{2}y = 0 \quad (**)$

From equation $(*)$, let's express $y$ in terms of $x$:
$2y = -\frac{1 - \sqrt{17}}{2}x$
$y = -\frac{1 - \sqrt{17}}{4}x = \frac{\sqrt{17} - 1}{4}x$

Let $x=4$. Then $y = \sqrt{17} - 1$.
So, an eigenvector for $\lambda_1$ is $v_1 = \begin{bmatrix} 4 \\ \sqrt{17} - 1 \end{bmatrix}$.
To normalize this eigenvector:
$\|v_1\| = \sqrt{4^2 + (\sqrt{17} - 1)^2} = \sqrt{16 + (17 - 2\sqrt{17} + 1)} = \sqrt{16 + 18 - 2\sqrt{17}} = \sqrt{34 - 2\sqrt{17}}$
$\approx \sqrt{34 - 2(4.123)} = \sqrt{34 - 8.246} = \sqrt{25.754} \approx 5.075$
Normalized eigenvector $\hat{v}_1 = \frac{1}{\sqrt{34 - 2\sqrt{17}}} \begin{bmatrix} 4 \\ \sqrt{17} - 1 \end{bmatrix} \approx \begin{bmatrix} 0.788 \\ 0.615 \end{bmatrix}$

#### For $\lambda_2 = \frac{7 - \sqrt{17}}{2}$:
Let $v_2 = \begin{bmatrix} x \\ y \end{bmatrix}$.
$\begin{bmatrix} 4-\lambda_2 & 2 \\ 2 & 3-\lambda_2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}$

Substitute $\lambda_2 = \frac{7 - \sqrt{17}}{2}$:
$4 - \frac{7 - \sqrt{17}}{2} = \frac{8 - 7 + \sqrt{17}}{2} = \frac{1 + \sqrt{17}}{2}$
$3 - \frac{7 - \sqrt{17}}{2} = \frac{6 - 7 + \sqrt{17}}{2} = \frac{-1 + \sqrt{17}}{2}$

The system of equations becomes:
$\frac{1 + \sqrt{17}}{2}x + 2y = 0 \quad (*')$
$2x + \frac{-1 + \sqrt{17}}{2}y = 0 \quad (**')$

From equation $(*')$:
$2y = -\frac{1 + \sqrt{17}}{2}x$
$y = -\frac{1 + \sqrt{17}}{4}x$

Let $x=4$. Then $y = -(1 + \sqrt{17})$.
So, an eigenvector for $\lambda_2$ is $v_2 = \begin{bmatrix} 4 \\ -(1 + \sqrt{17}) \end{bmatrix}$.
To normalize this eigenvector:
$\|v_2\| = \sqrt{4^2 + (-(1 + \sqrt{17}))^2} = \sqrt{16 + (1 + 2\sqrt{17} + 17)} = \sqrt{16 + 18 + 2\sqrt{17}} = \sqrt{34 + 2\sqrt{17}}$
$\approx \sqrt{34 + 8.246} = \sqrt{42.246} \approx 6.500$
Normalized eigenvector $\hat{v}_2 = \frac{1}{\sqrt{34 + 2\sqrt{17}}} \begin{bmatrix} 4 \\ -(1 + \sqrt{17}) \end{bmatrix} \approx \begin{bmatrix} 0.615 \\ -0.788 \end{bmatrix}$

We can confirm orthogonality of eigenvectors for a symmetric matrix:
$\hat{v}_1 \cdot \hat{v}_2 = \left(\frac{4}{\sqrt{34 - 2\sqrt{17}}}\right)\left(\frac{4}{\sqrt{34 + 2\sqrt{17}}}\right) + \left(\frac{\sqrt{17} - 1}{\sqrt{34 - 2\sqrt{17}}}\right)\left(\frac{-(1 + \sqrt{17})}{\sqrt{34 + 2\sqrt{17}}}\right)$
The numerator part: $16 - (\sqrt{17}-1)(\sqrt{17}+1) = 16 - (17-1) = 16 - 16 = 0$.
So the dot product is 0, confirming orthogonality.

### 3. Perform eigenvalue decomposition: represent A in the form $A = Q\Lambda Q^\top$.

Here:
* $\Lambda$ is the diagonal matrix of eigenvalues.
* $Q$ is the orthogonal matrix whose columns are the normalized eigenvectors. $Q^\top$ is the transpose of $Q$.

$\Lambda = \begin{bmatrix} \lambda_1 & 0 \\ 0 & \lambda_2 \end{bmatrix} = \begin{bmatrix} \frac{7 + \sqrt{17}}{2} & 0 \\ 0 & \frac{7 - \sqrt{17}}{2} \end{bmatrix} \approx \begin{bmatrix} 5.5615 & 0 \\ 0 & 1.4385 \end{bmatrix}$

$Q = [\hat{v}_1 \ \hat{v}_2] = \begin{bmatrix} \frac{4}{\sqrt{34 - 2\sqrt{17}}} & \frac{4}{\sqrt{34 + 2\sqrt{17}}} \\ \frac{\sqrt{17} - 1}{\sqrt{34 - 2\sqrt{17}}} & \frac{-(1 + \sqrt{17})}{\sqrt{34 + 2\sqrt{17}}} \end{bmatrix} \approx \begin{bmatrix} 0.788 & 0.615 \\ 0.615 & -0.788 \end{bmatrix}$

$Q^\top = \begin{bmatrix} \frac{4}{\sqrt{34 - 2\sqrt{17}}} & \frac{\sqrt{17} - 1}{\sqrt{34 - 2\sqrt{17}}} \\ \frac{4}{\sqrt{34 + 2\sqrt{17}}} & \frac{-(1 + \sqrt{17})}{\sqrt{34 + 2\sqrt{17}}} \end{bmatrix} \approx \begin{bmatrix} 0.788 & 0.615 \\ 0.615 & -0.788 \end{bmatrix}$

Thus, $A = Q\Lambda Q^\top$.
Let's verify with approximate values:
$Q\Lambda = \begin{bmatrix} 0.788 & 0.615 \\ 0.615 & -0.788 \end{bmatrix} \begin{bmatrix} 5.5615 & 0 \\ 0 & 1.4385 \end{bmatrix} = \begin{bmatrix} 0.788 \times 5.5615 & 0.615 \times 1.4385 \\ 0.615 \times 5.5615 & -0.788 \times 1.4385 \end{bmatrix} = \begin{bmatrix} 4.380 & 0.885 \\ 3.418 & -1.133 \end{bmatrix}$

$Q\Lambda Q^\top = \begin{bmatrix} 4.380 & 0.885 \\ 3.418 & -1.133 \end{bmatrix} \begin{bmatrix} 0.788 & 0.615 \\ 0.615 & -0.788 \end{bmatrix}$
$= \begin{bmatrix} (4.380)(0.788) + (0.885)(0.615) & (4.380)(0.615) + (0.885)(-0.788) \\ (3.418)(0.788) + (-1.133)(0.615) & (3.418)(0.615) + (-1.133)(-0.788) \end{bmatrix}$
$= \begin{bmatrix} 3.451 + 0.544 & 2.697 - 0.697 \\ 2.693 - 0.697 & 2.100 + 0.893 \end{bmatrix} = \begin{bmatrix} 3.995 & 2.000 \\ 1.996 & 2.993 \end{bmatrix}$
This is approximately $\begin{bmatrix} 4 & 2 \\ 2 & 3 \end{bmatrix}$, confirming the decomposition. (Differences are due to rounding approximations of $\sqrt{17}$).

---

### 4. Explain how eigenvalue decomposition can be used for dimensionality reduction in machine learning.

Eigenvalue decomposition is a fundamental technique for dimensionality reduction, most prominently applied in **Principal Component Analysis (PCA)**. Here's how it works:

1.  **Understanding Variance and Directions:**
    * In machine learning, data features often exhibit correlations. A covariance matrix (like $A$ in this problem) captures these relationships, where diagonal elements represent variances of individual features and off-diagonal elements represent covariances between pairs of features.
    * **Eigenvectors** represent the principal components (new axes) of the data. These are the directions along which the data varies the most. Importantly, these eigenvectors are orthogonal, meaning they represent uncorrelated dimensions in the transformed space.
    * **Eigenvalues** quantify the amount of variance in the data along their corresponding eigenvectors (principal components). A larger eigenvalue means that its corresponding eigenvector captures more of the total variance in the dataset.

2.  **The Process of Dimensionality Reduction (PCA):**
    * **Compute Covariance Matrix:** Start by computing the covariance matrix of your feature data (assuming features are centered, i.e., have zero mean).
    * **Eigenvalue Decomposition:** Perform eigenvalue decomposition on this covariance matrix. This yields a set of eigenvalues and their corresponding eigenvectors.
    * **Rank Eigenvalues:** Sort the eigenvalues in descending order. The eigenvector associated with the largest eigenvalue is the first principal component, capturing the most variance. The second largest eigenvalue corresponds to the second principal component, and so on.
    * **Select Top Principal Components:** To reduce dimensionality, you select only the top $k$ eigenvectors (principal components) that correspond to the largest eigenvalues. This means you retain the directions that explain the most variance in your data, effectively discarding the directions that account for less variance.
    * **Transform Data:** Project the original high-dimensional data onto the subspace spanned by these $k$ selected principal components. This transformation effectively reduces the number of features from $d$ to $k$ ($k < d$), while retaining as much information (variance) as possible.

3.  **Significance in Machine Learning:**
    * **Reduced Complexity:** Lower dimensionality means fewer features, which reduces the computational cost and training time for machine learning models.
    * **Mitigation of Curse of Dimensionality:** High-dimensional data often suffers from the "curse of dimensionality," where data becomes sparse and models struggle to generalize. Dimensionality reduction helps mitigate this by projecting data into a lower-dimensional, denser subspace.
    * **Noise Reduction:** Principal components with small eigenvalues often correspond to noise or less informative variations in the data. By discarding these components, PCA can effectively denoise the data.
    * **Improved Model Performance:** For many algorithms (e.g., K-Nearest Neighbors, SVMs, neural networks), reducing dimensionality can lead to better generalization by removing irrelevant or noisy features and focusing on the most important underlying patterns.
    * **Visualization:** For high-dimensional data (e.g., 100 features), it's impossible to visualize. PCA can reduce data to 2 or 3 dimensions, allowing for easy plotting and visual inspection of clusters or patterns.

In summary, eigenvalue decomposition allows us to identify the inherent structure and variability within a dataset. By leveraging the principal components (eigenvectors) that capture the most variance (quantified by eigenvalues), we can effectively reduce the dimensionality of the data while preserving its most critical information for machine learning tasks.